[2024-07-15T09:30:14.533+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:30:14.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:30:14.538+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:30:14.537+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:30:14.601+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:30:14.595+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
ImportError: cannot import name 'upload_s3_pipeline' from 'pipelines.aws_s3_pipeline' (/opt/airflow/pipelines/aws_s3_pipeline.py)
[2024-07-15T09:30:14.602+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:30:14.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.100 seconds
[2024-07-15T09:30:44.906+0000] {processor.py:161} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:30:44.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:30:44.908+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:30:44.908+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:30:44.946+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:30:44.941+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
ImportError: cannot import name 'upload_s3_pipeline' from 'pipelines.aws_s3_pipeline' (/opt/airflow/pipelines/aws_s3_pipeline.py)
[2024-07-15T09:30:44.947+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:30:44.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-15T09:31:15.216+0000] {processor.py:161} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:31:15.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:31:15.219+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:31:15.219+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:31:15.257+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:31:15.252+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
ImportError: cannot import name 'upload_s3_pipeline' from 'pipelines.aws_s3_pipeline' (/opt/airflow/pipelines/aws_s3_pipeline.py)
[2024-07-15T09:31:15.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:31:15.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-15T09:31:45.549+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:31:45.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:31:45.553+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:31:45.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:31:45.584+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:31:45.581+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
ImportError: cannot import name 'upload_s3_pipeline' from 'pipelines.aws_s3_pipeline' (/opt/airflow/pipelines/aws_s3_pipeline.py)
[2024-07-15T09:31:45.586+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:31:45.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-15T09:32:15.816+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:32:15.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:32:15.819+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:32:15.819+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:32:15.860+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:32:15.855+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
ImportError: cannot import name 'upload_s3_pipeline' from 'pipelines.aws_s3_pipeline' (/opt/airflow/pipelines/aws_s3_pipeline.py)
[2024-07-15T09:32:15.861+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:32:15.881+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T09:32:46.067+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:32:46.068+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:32:46.070+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:32:46.070+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:32:46.137+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:32:46.128+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:32:46.140+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:32:46.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-07-15T09:33:16.299+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:33:16.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:33:16.302+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:33:16.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:33:16.353+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:33:16.345+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:33:16.355+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:33:16.376+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.083 seconds
[2024-07-15T09:33:46.652+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:33:46.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:33:46.655+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:33:46.655+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:33:46.705+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:33:46.696+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:33:46.707+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:33:46.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-15T09:34:16.859+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:34:16.860+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:34:16.862+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:34:16.862+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:34:16.901+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:34:16.893+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:34:16.905+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:34:16.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-07-15T09:34:47.075+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:34:47.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:34:47.087+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:34:47.087+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:34:47.135+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:34:47.126+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:34:47.138+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:34:47.158+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.091 seconds
[2024-07-15T09:35:17.473+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:35:17.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:35:17.476+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:35:17.475+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:35:17.515+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:35:17.508+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:35:17.517+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:35:17.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-15T09:35:47.747+0000] {processor.py:161} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:35:47.748+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:35:47.751+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:35:47.750+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:35:47.790+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:35:47.782+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:35:47.792+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:35:47.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T09:36:18.028+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:36:18.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:36:18.035+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:36:18.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:36:18.154+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:36:18.135+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:36:18.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:36:18.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.257 seconds
[2024-07-15T09:36:48.511+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:36:48.513+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:36:48.515+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:36:48.514+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:36:48.552+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:36:48.546+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:36:48.554+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:36:48.580+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-15T09:37:18.740+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:37:18.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:37:18.744+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:37:18.744+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:37:18.797+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:37:18.787+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:37:18.800+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:37:18.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.089 seconds
[2024-07-15T09:37:49.049+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:37:49.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:37:49.053+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:37:49.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:37:49.106+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:37:49.096+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:37:49.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:37:49.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-15T09:38:19.288+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:38:19.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:38:19.291+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:38:19.291+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:38:19.341+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:38:19.334+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:38:19.343+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:38:19.364+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.086 seconds
[2024-07-15T09:38:49.647+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:38:49.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:38:49.655+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:38:49.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:38:49.700+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:38:49.693+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:38:49.702+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:38:49.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.091 seconds
[2024-07-15T09:39:19.921+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:39:19.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:39:19.926+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:39:19.925+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:39:19.983+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:39:19.972+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:39:19.986+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:39:20.011+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.097 seconds
[2024-07-15T09:39:50.196+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:39:50.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:39:50.200+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:39:50.200+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:39:50.246+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:39:50.238+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:39:50.248+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:39:50.269+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.080 seconds
[2024-07-15T09:40:20.475+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:40:20.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:40:20.478+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:40:20.478+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:40:20.528+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:40:20.519+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:40:20.531+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:40:20.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.084 seconds
[2024-07-15T09:40:50.767+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:40:50.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:40:50.770+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:40:50.770+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:40:50.806+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:40:50.801+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:40:50.807+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:40:50.829+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-15T09:41:21.124+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:41:21.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:41:21.127+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:41:21.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:41:21.166+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:41:21.161+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:41:21.168+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:41:21.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-15T09:41:40.350+0000] {processor.py:161} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:41:40.351+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:41:40.353+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:41:40.353+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:41:40.414+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:41:40.404+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:41:40.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:41:40.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.092 seconds
[2024-07-15T09:42:10.616+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:42:10.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:42:10.620+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:42:10.619+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:42:10.658+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:42:10.652+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:42:10.660+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:42:10.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-15T09:42:40.910+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:42:40.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:42:40.914+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:42:40.913+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:42:40.950+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:42:40.944+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:42:40.951+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:42:40.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-15T09:43:11.151+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:43:11.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:43:11.155+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:43:11.155+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:43:11.187+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:43:11.183+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:43:11.189+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:43:11.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-15T09:43:41.400+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:43:41.401+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:43:41.403+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:43:41.403+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:43:41.436+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:43:41.430+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:43:41.438+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:43:41.458+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-15T09:44:11.597+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:44:11.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:44:11.601+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:44:11.600+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:44:11.648+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:44:11.638+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:44:11.652+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:44:11.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T09:44:41.877+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:44:41.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:44:41.881+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:44:41.881+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:44:41.929+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:44:41.919+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:44:41.932+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:44:41.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.083 seconds
[2024-07-15T09:45:12.099+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:45:12.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:45:12.103+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:45:12.103+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:45:12.159+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:45:12.149+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:45:12.162+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:45:12.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.094 seconds
[2024-07-15T09:45:42.360+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:45:42.362+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:45:42.364+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:45:42.363+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:45:42.422+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:45:42.410+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:45:42.425+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:45:42.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.093 seconds
[2024-07-15T09:46:12.584+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:46:12.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:46:12.588+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:46:12.588+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:46:12.640+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:46:12.630+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:46:12.642+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:46:12.666+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.089 seconds
[2024-07-15T09:46:42.831+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:46:42.833+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:46:42.835+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:46:42.834+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:46:42.888+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:46:42.878+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:46:42.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:46:42.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.092 seconds
[2024-07-15T09:47:13.069+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:47:13.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:47:13.072+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:47:13.072+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:47:13.119+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:47:13.110+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:47:13.121+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:47:13.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.093 seconds
[2024-07-15T09:47:43.334+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:47:43.335+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:47:43.337+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:47:43.337+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:47:43.384+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:47:43.373+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:47:43.389+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:47:43.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.091 seconds
[2024-07-15T09:48:13.588+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:48:13.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:48:13.592+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:48:13.591+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:48:13.629+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:48:13.624+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:48:13.632+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:48:13.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-15T09:48:43.864+0000] {processor.py:161} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:48:43.866+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:48:43.869+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:48:43.868+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:48:43.905+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:48:43.900+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:48:43.907+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:48:43.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T09:49:14.122+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:49:14.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:49:14.126+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:49:14.126+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:49:14.179+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:49:14.172+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:49:14.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:49:14.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T09:49:44.399+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:49:44.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:49:44.403+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:49:44.402+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:49:44.450+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:49:44.444+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:49:44.453+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:49:44.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-15T09:50:14.646+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:50:14.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:50:14.649+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:50:14.649+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:50:14.703+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:50:14.692+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:50:14.706+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:50:14.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T09:50:44.990+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:50:44.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:50:44.994+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:50:44.993+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:50:45.044+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:50:45.033+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:50:45.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:50:45.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.096 seconds
[2024-07-15T09:51:15.245+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:51:15.246+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:51:15.248+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:51:15.248+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:51:15.300+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:51:15.291+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:51:15.304+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:51:15.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.088 seconds
[2024-07-15T09:51:45.618+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:51:45.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:51:45.623+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:51:45.622+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:51:45.662+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:51:45.657+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:51:45.664+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:51:45.685+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-15T09:52:15.875+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:52:15.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:52:15.878+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:52:15.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:52:15.927+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:52:15.919+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:52:15.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:52:15.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-15T09:52:46.149+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:52:46.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:52:46.152+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:52:46.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:52:46.202+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:52:46.193+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:52:46.205+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:52:46.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-15T09:53:16.407+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:53:16.409+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:53:16.411+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:53:16.410+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:53:16.445+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:53:16.440+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:53:16.447+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:53:16.467+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-15T09:53:46.659+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:53:46.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:53:46.662+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:53:46.662+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:53:46.707+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:53:46.699+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:53:46.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:53:46.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-15T09:54:16.915+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:54:16.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:54:16.918+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:54:16.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:54:16.954+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:54:16.948+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:54:16.955+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:54:16.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-15T09:54:47.217+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:54:47.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:54:47.221+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:54:47.221+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:54:47.271+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:54:47.262+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:54:47.274+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:54:47.294+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.084 seconds
[2024-07-15T09:55:17.543+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:55:17.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:55:17.547+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:55:17.546+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:55:17.591+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:55:17.584+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:55:17.593+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:55:17.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-15T09:55:47.915+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:55:47.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:55:47.918+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:55:47.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:55:47.954+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:55:47.948+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:55:47.956+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:55:47.982+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-15T09:56:18.331+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:56:18.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:56:18.334+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:56:18.334+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:56:18.368+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:56:18.362+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:56:18.370+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:56:18.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-15T09:56:48.571+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:56:48.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:56:48.574+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:56:48.573+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:56:48.616+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:56:48.610+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:56:48.617+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:56:48.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-15T09:57:18.983+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:57:18.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:57:18.986+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:57:18.986+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:57:19.023+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:57:19.018+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:57:19.025+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:57:19.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-15T09:57:49.262+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:57:49.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:57:49.265+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:57:49.264+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:57:49.300+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:57:49.294+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:57:49.303+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:57:49.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-15T09:58:19.616+0000] {processor.py:161} INFO - Started process (PID=86) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:58:19.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:58:19.629+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:58:19.629+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:58:19.706+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:58:19.697+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:58:19.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:58:19.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.129 seconds
[2024-07-15T09:58:49.827+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:58:49.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:58:49.831+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:58:49.831+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:58:49.867+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:58:49.862+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:58:49.868+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:58:49.891+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T09:59:20.083+0000] {processor.py:161} INFO - Started process (PID=88) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:59:20.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:59:20.088+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:59:20.087+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:59:20.135+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:59:20.128+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:59:20.137+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:59:20.156+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.080 seconds
[2024-07-15T09:59:50.312+0000] {processor.py:161} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:59:50.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T09:59:50.316+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:59:50.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:59:50.356+0000] {logging_mixin.py:188} INFO - [2024-07-15T09:59:50.349+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T09:59:50.358+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T09:59:50.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-15T10:00:20.816+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:00:20.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:00:20.819+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:00:20.819+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:00:20.858+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:00:20.852+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:00:20.859+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:00:20.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-15T10:00:51.094+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:00:51.095+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:00:51.097+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:00:51.097+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:00:51.144+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:00:51.137+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:00:51.146+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:00:51.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.077 seconds
[2024-07-15T10:01:21.469+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:01:21.470+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:01:21.472+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:01:21.472+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:01:21.512+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:01:21.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:01:21.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:01:21.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-15T10:01:51.823+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:01:51.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:01:51.826+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:01:51.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:01:51.868+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:01:51.861+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:01:51.869+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:01:51.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.077 seconds
[2024-07-15T10:02:22.154+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:02:22.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:02:22.158+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:02:22.157+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:02:22.199+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:02:22.193+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:02:22.201+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:02:22.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-15T10:02:52.394+0000] {processor.py:161} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:02:52.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:02:52.397+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:02:52.397+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:02:52.443+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:02:52.438+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:02:52.445+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:02:52.466+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-15T10:03:22.676+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:03:22.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:03:22.680+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:03:22.680+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:03:22.734+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:03:22.726+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:03:22.738+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:03:22.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.108 seconds
[2024-07-15T10:03:52.958+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:03:52.960+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:03:52.961+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:03:52.961+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:03:53.002+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:03:52.997+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:03:53.003+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:03:53.023+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-15T10:04:23.221+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:04:23.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:04:23.225+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:04:23.224+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:04:23.279+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:04:23.268+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:04:23.283+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:04:23.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-15T10:04:53.447+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:04:53.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:04:53.451+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:04:53.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:04:53.494+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:04:53.485+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:04:53.497+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:04:53.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-15T10:05:23.708+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:05:23.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:05:23.711+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:05:23.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:05:23.747+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:05:23.742+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:05:23.749+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:05:23.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-15T10:05:53.984+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:05:53.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:05:53.988+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:05:53.987+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:05:54.026+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:05:54.019+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:05:54.029+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:05:54.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-07-15T10:06:24.299+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:06:24.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:06:24.303+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:06:24.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:06:24.341+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:06:24.337+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:06:24.343+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:06:24.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-15T10:06:54.539+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:06:54.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:06:54.542+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:06:54.542+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:06:54.583+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:06:54.577+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:06:54.584+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:06:54.605+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-15T10:07:24.804+0000] {processor.py:161} INFO - Started process (PID=104) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:07:24.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:07:24.809+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:07:24.808+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:07:24.850+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:07:24.844+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:07:24.852+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:07:24.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-15T10:07:55.041+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:07:55.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:07:55.044+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:07:55.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:07:55.086+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:07:55.077+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:07:55.126+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:07:55.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.116 seconds
[2024-07-15T10:08:25.338+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:08:25.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:08:25.341+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:08:25.341+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:08:25.378+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:08:25.373+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:08:25.380+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:08:25.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-15T10:08:55.595+0000] {processor.py:161} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:08:55.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:08:55.599+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:08:55.598+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:08:55.636+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:08:55.629+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:08:55.638+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:08:55.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-15T10:09:25.822+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:09:25.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:09:25.826+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:09:25.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:09:25.869+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:09:25.860+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:09:25.873+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:09:25.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-15T10:09:56.171+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:09:56.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:09:56.177+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:09:56.176+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:09:56.246+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:09:56.234+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:09:56.249+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:09:56.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.118 seconds
[2024-07-15T10:10:26.390+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:10:26.391+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:10:26.395+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:10:26.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:10:26.427+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:10:26.422+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:10:26.428+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:10:26.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-15T10:10:56.669+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:10:56.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:10:56.673+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:10:56.672+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:10:56.731+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:10:56.721+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:10:56.734+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:10:56.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.098 seconds
[2024-07-15T10:11:26.857+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:11:26.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:11:26.860+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:11:26.859+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:11:26.908+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:11:26.900+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:11:26.910+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:11:26.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.089 seconds
[2024-07-15T10:11:57.145+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:11:57.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:11:57.152+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:11:57.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:11:57.220+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:11:57.210+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:11:57.222+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:11:57.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.117 seconds
[2024-07-15T10:12:27.527+0000] {processor.py:161} INFO - Started process (PID=114) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:12:27.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:12:27.541+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:12:27.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:12:27.602+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:12:27.587+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:12:27.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:07.317+0000] {processor.py:161} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:07.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:13:07.335+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:13:07.334+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:07.485+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:13:07.430+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:13:07.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:07.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.258 seconds
[2024-07-15T10:13:37.771+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:37.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:13:37.775+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:13:37.774+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:37.827+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:13:37.817+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:13:37.831+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:13:37.852+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.088 seconds
[2024-07-15T10:14:08.013+0000] {processor.py:161} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:14:08.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:14:08.017+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:14:08.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:14:08.069+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:14:08.059+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:14:08.073+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:14:08.097+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.091 seconds
[2024-07-15T10:14:38.335+0000] {processor.py:161} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:14:38.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:14:38.338+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:14:38.337+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:14:38.377+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:14:38.372+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:14:38.379+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:14:38.401+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-15T10:15:08.580+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:15:08.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:15:08.584+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:15:08.584+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:15:08.654+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:15:08.645+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:15:08.657+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:15:08.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.108 seconds
[2024-07-15T10:15:38.844+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:15:38.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:15:38.847+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:15:38.846+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:15:38.894+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:15:38.887+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:15:38.896+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:15:38.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.080 seconds
[2024-07-15T10:16:09.129+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:16:09.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:16:09.132+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:16:09.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:16:09.184+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:16:09.173+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:16:09.187+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:16:09.211+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.089 seconds
[2024-07-15T10:16:39.380+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:16:39.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:16:39.386+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:16:39.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:16:39.423+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:16:39.416+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
ModuleNotFoundError: No module named 'etls.aws_etl'
[2024-07-15T10:16:39.425+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:16:39.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.077 seconds
[2024-07-15T10:17:09.657+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:17:09.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:17:09.666+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:17:09.665+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:17:09.725+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:17:09.712+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:17:09.728+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:17:09.748+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.098 seconds
[2024-07-15T10:17:39.885+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:17:39.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:17:39.888+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:17:39.887+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:17:39.946+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:17:39.932+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:17:39.949+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:17:39.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.094 seconds
[2024-07-15T10:18:10.217+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:18:10.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:18:10.220+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:18:10.220+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:18:10.260+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:18:10.252+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:18:10.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:18:10.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T10:18:40.507+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:18:40.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:18:40.512+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:18:40.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:18:40.576+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:18:40.561+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:18:40.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:18:40.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.110 seconds
[2024-07-15T10:19:10.742+0000] {processor.py:161} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:19:10.743+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:19:10.745+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:19:10.745+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:19:10.795+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:19:10.785+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:19:10.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:19:10.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-15T10:19:41.074+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:19:41.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:19:41.077+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:19:41.077+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:19:41.119+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:19:41.110+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:19:41.121+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:19:41.139+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T10:20:11.344+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:20:11.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:20:11.348+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:20:11.347+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:20:11.394+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:20:11.386+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:20:11.397+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:20:11.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.080 seconds
[2024-07-15T10:20:41.591+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:20:41.593+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:20:41.597+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:20:41.596+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:20:41.648+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:20:41.634+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:20:41.652+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:20:41.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.108 seconds
[2024-07-15T10:21:11.866+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:21:11.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:21:11.869+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:21:11.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:21:11.913+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:21:11.903+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:21:11.915+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:21:11.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-15T10:21:42.161+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:21:42.162+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:21:42.164+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:21:42.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:21:42.209+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:21:42.202+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:21:42.210+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:21:42.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.077 seconds
[2024-07-15T10:22:12.439+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:22:12.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:22:12.442+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:22:12.441+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:22:12.485+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:22:12.476+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:22:12.488+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:22:12.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.089 seconds
[2024-07-15T10:22:42.624+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:22:42.626+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:22:42.629+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:22:42.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:22:42.688+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:22:42.677+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:22:42.691+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:22:42.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-07-15T10:23:12.947+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:23:12.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:23:12.950+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:23:12.950+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:23:13.019+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:23:13.005+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:23:13.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:23:13.041+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.101 seconds
[2024-07-15T10:23:43.208+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:23:43.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:23:43.212+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:23:43.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:23:43.258+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:23:43.248+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:23:43.260+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:23:43.287+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T10:24:13.449+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:24:13.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:24:13.454+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:24:13.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:24:13.514+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:24:13.501+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:24:13.517+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:24:13.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-07-15T10:24:43.742+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:24:43.743+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:24:43.746+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:24:43.745+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:24:43.794+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:24:43.785+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:24:43.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:24:43.817+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-15T10:25:13.982+0000] {processor.py:161} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:25:13.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:25:13.985+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:25:13.985+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:25:14.036+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:25:14.027+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:25:14.038+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:25:14.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T10:25:44.202+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:25:44.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:25:44.205+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:25:44.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:25:44.258+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:25:44.245+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:25:44.260+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:25:44.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T10:26:14.505+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:26:14.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:26:14.508+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:26:14.508+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:26:14.548+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:26:14.541+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:26:14.549+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:26:14.570+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T10:26:44.759+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:26:44.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:26:44.762+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:26:44.762+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:26:44.816+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:26:44.805+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:26:44.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:26:44.835+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-15T10:27:15.098+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:27:15.099+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:27:15.101+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:27:15.101+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:27:15.150+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:27:15.137+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:27:15.152+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:27:15.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-07-15T10:27:45.362+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:27:45.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:27:45.366+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:27:45.366+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:27:45.418+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:27:45.408+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:27:45.421+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:27:45.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.089 seconds
[2024-07-15T10:28:15.645+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:28:15.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:28:15.649+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:28:15.648+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:28:15.712+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:28:15.699+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:28:15.714+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:28:15.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.097 seconds
[2024-07-15T10:28:45.873+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:28:45.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:28:45.877+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:28:45.876+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:28:45.937+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:28:45.924+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:28:45.940+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:28:45.961+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.094 seconds
[2024-07-15T10:29:16.176+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:29:16.177+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:29:16.179+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:29:16.179+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:29:16.228+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:29:16.217+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:29:16.229+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:29:16.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T10:29:46.434+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:29:46.435+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:29:46.437+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:29:46.436+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:29:46.486+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:29:46.476+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:29:46.489+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:29:46.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-15T10:30:16.724+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:30:16.725+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:30:16.727+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:30:16.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:30:16.772+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:30:16.762+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:30:16.775+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:30:16.802+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.084 seconds
[2024-07-15T10:30:46.982+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:30:46.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:30:46.986+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:30:46.985+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:30:47.033+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:30:47.024+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:30:47.035+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:30:47.055+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-15T10:31:17.239+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:31:17.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:31:17.242+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:31:17.242+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:31:17.301+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:31:17.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:31:17.303+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:31:17.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.097 seconds
[2024-07-15T10:31:47.488+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:31:47.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:31:47.491+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:31:47.491+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:31:47.554+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:31:47.541+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:31:47.556+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:31:47.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.102 seconds
[2024-07-15T10:32:17.886+0000] {processor.py:161} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:32:17.887+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:32:17.889+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:32:17.889+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:32:17.929+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:32:17.922+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:32:17.931+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:32:17.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-15T10:32:48.090+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:32:48.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:32:48.093+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:32:48.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:32:48.131+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:32:48.125+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:32:48.132+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:32:48.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-15T10:33:18.413+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:33:18.414+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:33:18.416+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:33:18.416+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:33:18.467+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:33:18.454+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:33:18.469+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:33:18.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.086 seconds
[2024-07-15T10:36:10.370+0000] {processor.py:161} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:36:10.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:36:10.374+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:36:10.373+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:36:10.434+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:36:10.420+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:36:10.438+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:36:10.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-07-15T10:36:40.688+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:36:40.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:36:40.696+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:36:40.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:36:40.747+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:36:40.740+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:36:40.749+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:36:40.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.137 seconds
[2024-07-15T10:37:10.936+0000] {processor.py:161} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:37:10.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:37:10.939+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:37:10.939+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:37:10.998+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:37:10.985+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:37:11.000+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:37:11.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.096 seconds
[2024-07-15T10:37:41.198+0000] {processor.py:161} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:37:41.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:37:41.205+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:37:41.205+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:37:41.263+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:37:41.250+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:37:41.265+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:37:41.288+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.103 seconds
[2024-07-15T10:38:11.446+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:38:11.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:38:11.448+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:38:11.448+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:38:11.488+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:38:11.479+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:38:11.490+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:38:11.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-15T10:38:41.759+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:38:41.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:38:41.765+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:38:41.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:38:41.840+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:38:41.820+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:38:41.845+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:38:41.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.121 seconds
[2024-07-15T10:39:12.010+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:39:12.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:39:12.017+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:39:12.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:39:12.082+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:39:12.069+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:39:12.086+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:39:12.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.114 seconds
[2024-07-15T10:39:42.306+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:39:42.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:39:42.310+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:39:42.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:39:42.373+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:39:42.358+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:39:42.377+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:39:42.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.100 seconds
[2024-07-15T10:40:12.526+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:40:12.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:40:12.530+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:40:12.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:40:12.577+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:40:12.566+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:40:12.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:40:12.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.085 seconds
[2024-07-15T10:40:42.820+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:40:42.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:40:42.823+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:40:42.823+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:40:42.878+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:40:42.865+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:40:42.880+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:40:42.902+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-15T10:41:13.097+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:41:13.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:41:13.101+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:41:13.100+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:41:13.170+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:41:13.154+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:41:13.173+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:41:13.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.112 seconds
[2024-07-15T10:41:43.337+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:41:43.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:41:43.340+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:41:43.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:41:43.392+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:41:43.381+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:41:43.395+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:41:43.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.084 seconds
[2024-07-15T10:42:13.632+0000] {processor.py:161} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:42:13.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:42:13.639+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:42:13.638+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:42:13.757+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:42:13.740+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:42:13.761+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:42:13.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.163 seconds
[2024-07-15T10:42:43.870+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:42:43.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:42:43.874+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:42:43.873+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:42:43.913+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:42:43.904+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:42:43.915+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:42:43.958+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.095 seconds
[2024-07-15T10:43:11.089+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:43:11.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:43:11.093+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:43:11.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:43:11.145+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:43:11.137+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:43:11.147+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:43:11.167+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-15T10:43:41.417+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:43:41.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:43:41.421+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:43:41.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:43:41.479+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:43:41.466+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:43:41.482+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:43:41.509+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-07-15T10:44:11.637+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:11.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:44:11.640+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:11.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:11.697+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:11.685+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:44:11.700+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:11.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-15T10:44:41.946+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:41.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:44:41.950+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:41.950+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:41.994+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:41.987+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 10, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 1, in <module>
    from etls.aws_etl import connect_to_s3, create_bucket_if_not_exist, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-15T10:44:41.996+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:42.018+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-15T10:44:55.135+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:55.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:44:55.138+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:55.286+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:44:55.288+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:44:55.352+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:44:55.353+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:44:55.641+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:44:55.787+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.787+0000] {override.py:1858} INFO - Created Permission View: can edit on DAG:etl_reddit_pipeline
[2024-07-15T10:44:55.798+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.798+0000] {override.py:1858} INFO - Created Permission View: can delete on DAG:etl_reddit_pipeline
[2024-07-15T10:44:55.806+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.805+0000] {override.py:1858} INFO - Created Permission View: can read on DAG:etl_reddit_pipeline
[2024-07-15T10:44:55.807+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.806+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:44:55.817+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.817+0000] {dag.py:3118} INFO - Creating ORM DAG for etl_reddit_pipeline
[2024-07-15T10:44:55.828+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:44:55.828+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:44:55.853+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.725 seconds
[2024-07-15T10:45:26.075+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:45:26.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:45:26.079+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:45:26.079+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:45:26.157+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:45:26.158+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:45:26.208+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:45:26.209+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:45:26.477+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:45:26.489+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:45:26.489+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:45:26.528+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:45:26.527+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:45:26.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.486 seconds
[2024-07-15T10:45:56.786+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:45:56.787+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:45:56.790+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:45:56.789+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:45:56.868+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:45:56.869+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:45:56.923+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:45:56.924+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:45:57.205+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:45:57.341+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:45:57.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:45:57.371+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:45:57.370+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:45:57.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.641 seconds
[2024-07-15T10:46:27.784+0000] {processor.py:161} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:46:27.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:46:27.789+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:46:27.788+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:46:27.870+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:46:27.871+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:46:27.932+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:46:27.934+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:46:28.211+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:46:28.242+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:46:28.242+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:46:28.271+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:46:28.270+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:46:28.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.522 seconds
[2024-07-15T10:46:58.549+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:46:58.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:46:58.553+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:46:58.553+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:46:58.630+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:46:58.631+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:46:58.687+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:46:58.688+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:46:58.946+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:46:58.973+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:46:58.972+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:46:58.994+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:46:58.994+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:46:59.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.481 seconds
[2024-07-15T10:47:29.258+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:47:29.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:47:29.262+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:47:29.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:47:29.343+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:47:29.343+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:47:29.400+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:47:29.401+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:47:29.813+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:47:29.844+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:47:29.844+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:47:29.867+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:47:29.867+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:47:29.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-15T10:48:00.049+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:48:00.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:48:00.053+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:48:00.053+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:48:00.122+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:48:00.122+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:48:00.175+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:48:00.176+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:48:00.431+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:48:00.456+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:48:00.455+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:48:00.478+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:48:00.478+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:48:00.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.456 seconds
[2024-07-15T10:48:30.778+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:48:30.779+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:48:30.783+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:48:30.783+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:48:30.872+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:48:30.873+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:48:30.931+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:48:30.932+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:48:31.246+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:48:31.274+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:48:31.274+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:48:31.304+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:48:31.303+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:48:31.329+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.561 seconds
[2024-07-15T10:49:01.541+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:49:01.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:49:01.545+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:49:01.545+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:49:01.629+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:49:01.630+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:49:01.687+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:49:01.687+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:49:01.953+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:49:01.979+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:49:01.978+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:49:02.002+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:49:02.001+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:49:02.023+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-07-15T10:49:32.282+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:49:32.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:49:32.287+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:49:32.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:49:32.398+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:49:32.399+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:49:32.524+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:49:32.526+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:49:32.836+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:49:32.860+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:49:32.860+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:49:32.883+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:49:32.882+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:49:32.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-15T10:50:02.983+0000] {processor.py:161} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:50:02.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:50:02.986+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:50:02.986+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:50:03.080+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:50:03.080+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:50:03.136+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:50:03.137+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:50:03.404+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:50:03.432+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:50:03.431+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:50:03.456+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:50:03.456+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:50:03.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.500 seconds
[2024-07-15T10:50:33.775+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:50:33.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:50:33.790+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:50:33.790+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:50:33.906+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:50:33.907+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:50:33.963+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:50:33.964+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:50:34.226+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:50:34.251+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:50:34.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:50:34.272+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:50:34.272+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:50:34.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.562 seconds
[2024-07-15T10:51:04.545+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:51:04.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:51:04.549+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:51:04.549+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:51:04.623+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:51:04.623+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:51:04.676+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:51:04.677+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:51:05.000+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:51:05.028+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:51:05.028+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:51:05.052+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:51:05.051+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:51:05.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.537 seconds
[2024-07-15T10:51:35.264+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:51:35.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:51:35.268+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:51:35.268+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:51:35.354+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:51:35.355+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:51:35.415+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:51:35.415+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:51:35.680+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:51:35.710+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:51:35.710+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:51:35.737+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:51:35.736+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:51:35.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.508 seconds
[2024-07-15T10:52:06.031+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:52:06.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:52:06.034+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:52:06.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:52:06.106+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:52:06.106+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:52:06.157+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:52:06.158+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:52:06.424+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:52:06.449+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:52:06.448+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:52:06.471+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:52:06.471+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:52:06.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.479 seconds
[2024-07-15T10:52:36.730+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:52:36.732+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:52:36.733+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:52:36.733+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:52:36.800+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:52:36.801+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:52:36.855+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:52:36.855+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:52:37.125+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:52:37.154+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:52:37.154+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:52:37.177+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:52:37.177+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:52:37.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.475 seconds
[2024-07-15T10:53:07.425+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:53:07.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:53:07.433+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:53:07.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:53:07.489+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:53:07.489+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:53:07.543+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:53:07.544+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:53:07.797+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:53:07.823+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:53:07.823+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:53:07.845+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:53:07.845+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:53:07.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.449 seconds
[2024-07-15T10:53:38.123+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:53:38.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:53:38.126+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:53:38.125+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:53:38.196+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:53:38.196+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:53:38.248+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:53:38.249+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:53:38.501+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:53:38.527+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:53:38.526+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:53:38.549+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:53:38.549+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:53:38.571+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.457 seconds
[2024-07-15T10:54:08.811+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:54:08.812+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:54:08.815+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:54:08.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:54:08.882+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:54:08.883+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:54:08.941+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:54:08.943+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:54:09.232+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:54:09.261+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:54:09.260+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:54:09.284+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:54:09.284+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:54:09.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.505 seconds
[2024-07-15T10:54:39.530+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:54:39.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:54:39.533+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:54:39.533+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:54:39.604+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:54:39.605+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:54:39.659+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:54:39.659+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:54:39.901+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:54:39.927+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:54:39.926+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T10:54:39.950+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:54:39.950+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T10:54:39.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.449 seconds
[2024-07-15T10:55:12.681+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:55:12.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T10:55:12.758+0000] {logging_mixin.py:188} INFO - [2024-07-15T10:55:12.757+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T10:55:14.134+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:55:14.143+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T10:55:14.833+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T10:55:14.841+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:02:47.906+0000] {processor.py:161} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:02:47.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:02:47.909+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:02:47.909+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:02:50.663+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:02:50.667+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:02:50.925+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:02:50.925+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:02:52.239+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:02:52.734+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:02:52.733+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:02:52.813+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:02:52.813+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:02:52.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 5.026 seconds
[2024-07-15T17:03:23.093+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:03:23.095+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:03:23.097+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:03:23.097+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:03:23.232+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:03:23.233+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:03:23.359+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:03:23.359+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:03:23.902+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:03:23.938+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:03:23.937+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:03:23.984+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:03:23.983+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:03:24.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.934 seconds
[2024-07-15T17:03:54.282+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:03:54.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:03:54.295+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:03:54.294+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:03:54.425+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:03:54.426+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:03:54.611+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:03:54.612+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:03:55.237+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:03:55.276+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:03:55.275+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:03:55.312+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:03:55.312+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:03:55.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.083 seconds
[2024-07-15T17:04:25.615+0000] {processor.py:161} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:04:25.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:04:25.619+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:04:25.619+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:04:25.781+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:04:25.783+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:04:25.913+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:04:25.914+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:04:26.515+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:04:26.626+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:04:26.625+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:04:26.666+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:04:26.666+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:04:26.695+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.096 seconds
[2024-07-15T17:04:57.176+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:04:57.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:04:57.195+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:04:57.193+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:04:57.434+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:04:57.435+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:04:57.552+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:04:57.554+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:04:58.273+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:04:58.351+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:04:58.350+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:04:58.409+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:04:58.409+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:04:58.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.376 seconds
[2024-07-15T17:05:29.038+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:05:29.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:05:29.047+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:05:29.046+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:05:29.267+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:05:29.268+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:05:29.356+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:05:29.357+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:05:29.871+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:05:29.965+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:05:29.963+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:05:30.058+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:05:30.058+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:05:30.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.096 seconds
[2024-07-15T17:06:00.438+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:06:00.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:06:00.446+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:06:00.445+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:06:00.715+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:06:00.726+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:06:00.898+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:06:00.899+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:06:01.592+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:06:01.652+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:06:01.651+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:06:01.706+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:06:01.706+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:06:01.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.349 seconds
[2024-07-15T17:06:32.013+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:06:32.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:06:32.021+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:06:32.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:06:32.290+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:06:32.293+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:06:32.496+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:06:32.498+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:06:33.100+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:06:33.161+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:06:33.160+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:06:33.234+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:06:33.234+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:06:33.295+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.310 seconds
[2024-07-15T17:07:03.691+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:07:03.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:07:03.696+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:07:03.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:07:03.877+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:07:03.878+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:07:03.974+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:07:03.975+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:07:04.736+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:07:04.770+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:07:04.769+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:07:04.801+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:07:04.801+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:07:04.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.157 seconds
[2024-07-15T17:07:35.340+0000] {processor.py:161} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:07:35.346+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:07:35.352+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:07:35.350+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:07:35.629+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:07:35.632+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:07:35.793+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:07:35.794+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:07:36.645+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:07:36.760+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:07:36.758+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:07:36.870+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:07:36.869+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:07:36.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.653 seconds
[2024-07-15T17:08:07.511+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:08:07.514+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:08:07.517+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:08:07.516+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:08:07.653+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:08:07.654+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:08:07.749+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:08:07.751+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:08:08.529+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:08:08.619+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:08:08.617+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:08:08.707+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:08:08.707+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:08:08.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.291 seconds
[2024-07-15T17:08:39.218+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:08:39.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:08:39.228+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:08:39.226+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:08:39.479+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:08:39.481+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:08:39.654+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:08:39.656+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:08:40.541+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:08:40.624+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:08:40.623+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:08:40.729+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:08:40.728+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:08:40.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.627 seconds
[2024-07-15T17:09:11.424+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:09:11.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:09:11.431+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:09:11.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:09:11.619+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:09:11.621+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:09:11.781+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:09:11.782+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:09:12.374+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:09:12.431+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:09:12.430+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:09:12.470+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:09:12.470+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:09:12.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.109 seconds
[2024-07-15T17:09:42.970+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:09:42.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:09:42.980+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:09:42.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:09:43.274+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:09:43.277+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:09:43.532+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:09:43.534+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:09:44.078+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:09:44.139+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:09:44.138+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:09:44.202+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:09:44.202+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:09:44.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.335 seconds
[2024-07-15T17:10:14.472+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:10:14.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:10:14.480+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:10:14.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:10:14.742+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:10:14.744+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:10:14.953+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:10:14.955+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:10:15.677+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:10:15.774+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:10:15.773+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:10:15.875+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:10:15.874+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:10:15.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.506 seconds
[2024-07-15T17:10:46.251+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:10:46.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:10:46.255+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:10:46.254+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:10:46.335+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:10:46.335+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:10:46.406+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:10:46.407+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:10:46.925+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:10:46.987+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:10:46.986+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:10:47.044+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:10:47.044+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:10:47.091+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.856 seconds
[2024-07-15T17:11:17.690+0000] {processor.py:161} INFO - Started process (PID=121) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:11:17.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:11:17.699+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:11:17.698+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:11:17.917+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:11:17.919+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:11:18.023+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:11:18.025+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:11:18.633+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:11:18.670+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:11:18.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:11:18.716+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:11:18.716+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:11:18.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.102 seconds
[2024-07-15T17:11:49.158+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:11:49.162+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:11:49.167+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:11:49.165+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:11:49.411+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:11:49.412+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:11:49.532+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:11:49.533+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:11:49.992+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:11:50.039+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:11:50.037+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:11:50.073+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:11:50.073+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:11:50.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.978 seconds
[2024-07-15T17:12:20.627+0000] {processor.py:161} INFO - Started process (PID=125) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:12:20.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:12:20.636+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:12:20.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:12:20.880+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:12:20.881+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:12:20.952+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:12:20.954+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:12:21.581+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:12:21.680+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:12:21.678+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:12:21.733+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:12:21.732+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:12:21.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.171 seconds
[2024-07-15T17:12:52.232+0000] {processor.py:161} INFO - Started process (PID=127) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:12:52.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:12:52.241+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:12:52.240+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:12:52.479+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:12:52.480+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:12:52.569+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:12:52.570+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:12:53.106+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:12:53.140+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:12:53.140+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:12:53.170+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:12:53.170+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:12:53.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.995 seconds
[2024-07-15T17:13:23.516+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:13:23.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:13:23.519+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:13:23.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:13:23.606+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:13:23.608+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:13:23.740+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:13:23.741+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:13:24.206+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:13:24.289+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:13:24.288+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:13:24.389+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:13:24.388+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:13:24.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.961 seconds
[2024-07-15T17:13:54.953+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:13:54.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:13:54.957+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:13:54.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:13:55.109+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:13:55.111+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:13:55.217+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:13:55.218+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:13:55.759+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:13:55.822+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:13:55.821+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:13:55.880+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:13:55.879+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:13:55.937+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.996 seconds
[2024-07-15T17:14:26.493+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:14:26.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:14:26.502+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:14:26.501+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:14:26.749+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:14:26.751+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:14:26.883+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:14:26.884+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:14:27.645+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:14:27.753+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:14:27.752+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:14:27.845+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:14:27.845+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:14:27.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.491 seconds
[2024-07-15T17:14:58.499+0000] {processor.py:161} INFO - Started process (PID=135) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:14:58.503+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:14:58.508+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:14:58.506+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:14:58.772+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:14:58.775+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:14:58.923+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:14:58.925+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:14:59.830+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:14:59.901+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:14:59.900+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:14:59.955+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:14:59.955+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:15:00.006+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.532 seconds
[2024-07-15T17:15:30.341+0000] {processor.py:161} INFO - Started process (PID=137) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:15:30.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:15:30.350+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:15:30.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:15:30.565+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:15:30.566+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:15:30.717+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:15:30.719+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:15:31.369+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:15:31.456+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:15:31.455+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:15:31.508+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:15:31.507+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:15:31.550+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.236 seconds
[2024-07-15T17:16:01.987+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:16:01.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:16:01.999+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:16:01.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:16:02.169+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:16:02.171+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:16:02.302+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:16:02.304+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:16:02.750+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:16:02.795+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:16:02.795+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:16:02.854+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:16:02.853+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:16:02.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.948 seconds
[2024-07-15T17:16:33.475+0000] {processor.py:161} INFO - Started process (PID=141) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:16:33.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:16:33.483+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:16:33.482+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:16:33.728+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:16:33.730+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:16:33.881+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:16:33.883+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:16:34.497+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:16:34.560+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:16:34.559+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:16:34.619+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:16:34.618+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:16:34.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.224 seconds
[2024-07-15T17:17:04.962+0000] {processor.py:161} INFO - Started process (PID=143) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:17:04.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:17:04.972+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:17:04.970+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:17:05.233+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:17:05.235+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:17:05.438+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:17:05.440+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:17:06.090+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:17:06.157+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:17:06.156+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:17:06.231+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:17:06.230+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:17:06.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.355 seconds
[2024-07-15T17:17:36.651+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:17:36.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:17:36.660+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:17:36.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:17:36.910+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:17:36.912+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:17:37.054+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:17:37.056+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:17:37.532+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:17:37.593+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:17:37.593+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:17:37.650+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:17:37.650+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:17:37.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.081 seconds
[2024-07-15T17:18:08.400+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:18:08.404+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:18:08.408+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:18:08.407+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:18:08.693+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:18:08.696+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:18:08.896+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:18:08.899+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:18:09.742+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:18:09.840+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:18:09.839+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:18:09.925+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:18:09.925+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:18:10.005+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.637 seconds
[2024-07-15T17:18:40.539+0000] {processor.py:161} INFO - Started process (PID=149) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:18:40.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:18:40.548+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:18:40.547+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:18:40.816+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:18:40.818+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:18:40.976+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:18:40.977+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:18:41.660+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:18:41.718+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:18:41.717+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:18:41.773+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:18:41.773+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:18:41.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.313 seconds
[2024-07-15T17:19:12.282+0000] {processor.py:161} INFO - Started process (PID=151) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:19:12.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:19:12.290+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:19:12.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:19:12.593+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:19:12.596+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:19:12.752+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:19:12.754+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:19:13.377+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:19:13.424+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:19:13.423+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:19:13.502+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:19:13.501+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:19:13.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.334 seconds
[2024-07-15T17:19:43.823+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:19:43.827+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:19:43.831+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:19:43.830+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:19:44.052+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:19:44.053+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:19:44.129+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:19:44.130+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:19:44.625+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:19:44.721+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:19:44.719+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:19:44.796+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:19:44.795+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:19:44.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.051 seconds
[2024-07-15T17:25:58.909+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:25:58.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:25:58.922+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:25:58.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:25:59.642+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:25:59.643+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:26:00.022+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:26:00.031+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:26:01.712+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:26:01.843+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:26:01.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:26:02.003+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:26:02.002+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:26:02.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 3.263 seconds
[2024-07-15T17:26:32.439+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:26:32.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:26:32.443+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:26:32.442+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:26:32.533+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:26:32.534+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:26:32.593+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:26:32.594+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:26:32.881+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:26:32.906+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:26:32.906+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:26:32.930+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:26:32.930+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:26:32.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.527 seconds
[2024-07-15T17:27:03.207+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:27:03.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:27:03.212+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:27:03.211+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:27:03.311+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:27:03.312+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:27:03.410+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:27:03.411+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:27:03.811+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:27:03.876+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:27:03.873+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:27:03.943+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:27:03.943+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:27:04.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.808 seconds
[2024-07-15T17:27:34.189+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:27:34.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:27:34.201+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:27:34.199+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:27:34.541+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:27:34.544+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:27:34.787+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:27:34.790+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:27:35.810+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:27:35.867+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:27:35.866+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:27:35.929+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:27:35.929+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:27:35.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.837 seconds
[2024-07-15T17:28:06.617+0000] {processor.py:161} INFO - Started process (PID=163) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:28:06.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:28:06.621+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:28:06.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:28:06.781+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:28:06.783+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:28:06.964+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:28:06.965+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:28:07.752+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:28:07.805+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:28:07.804+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:28:07.855+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:28:07.855+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:28:07.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.306 seconds
[2024-07-15T17:28:38.262+0000] {processor.py:161} INFO - Started process (PID=165) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:28:38.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:28:38.267+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:28:38.266+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:28:38.376+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:28:38.378+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:28:38.482+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:28:38.483+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:28:38.957+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:28:39.015+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:28:39.014+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:28:39.062+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:28:39.062+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:28:39.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.879 seconds
[2024-07-15T17:29:09.790+0000] {processor.py:161} INFO - Started process (PID=167) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:29:09.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:29:09.800+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:29:09.798+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:29:10.053+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:29:10.055+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:29:10.179+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:29:10.181+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:29:10.733+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:29:10.771+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:29:10.771+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:29:10.811+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:29:10.810+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:29:10.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.119 seconds
[2024-07-15T17:29:41.417+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:29:41.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:29:41.421+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:29:41.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:29:41.572+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:29:41.574+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:29:41.792+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:29:41.793+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:29:42.358+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:29:42.455+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:29:42.445+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:29:42.533+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:29:42.533+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:29:42.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.193 seconds
[2024-07-15T17:30:12.996+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:30:13.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:30:13.006+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:30:13.005+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:30:13.163+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:30:13.164+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:30:13.279+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:30:13.280+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:30:14.047+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:30:14.110+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:30:14.109+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:30:14.187+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:30:14.186+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:30:14.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.279 seconds
[2024-07-15T17:30:44.487+0000] {processor.py:161} INFO - Started process (PID=173) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:30:44.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:30:44.493+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:30:44.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:30:44.674+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:30:44.676+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:30:44.815+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:30:44.817+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:30:45.563+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:30:45.617+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:30:45.616+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:30:45.668+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:30:45.668+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:30:45.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.246 seconds
[2024-07-15T17:31:16.293+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:31:16.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:31:16.308+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:31:16.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:31:16.607+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:31:16.609+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:31:16.784+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:31:16.785+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:31:17.639+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:31:17.686+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:31:17.685+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:31:17.738+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:31:17.737+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:31:17.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.574 seconds
[2024-07-15T17:31:48.465+0000] {processor.py:161} INFO - Started process (PID=177) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:31:48.469+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:31:48.474+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:31:48.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:31:48.762+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:31:48.764+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:31:49.001+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:31:49.004+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:31:50.315+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:31:50.439+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:31:50.436+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:31:50.543+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:31:50.542+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:31:50.629+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.199 seconds
[2024-07-15T17:32:20.987+0000] {processor.py:161} INFO - Started process (PID=179) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:32:20.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:32:20.992+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:32:20.991+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:32:21.129+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:32:21.130+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:32:21.249+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:32:21.250+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:32:21.825+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:32:21.873+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:32:21.872+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:32:21.912+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:32:21.911+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:32:21.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.972 seconds
[2024-07-15T17:32:52.488+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:32:52.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:32:52.502+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:32:52.500+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:32:52.812+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:32:52.816+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:32:53.048+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:32:53.050+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:32:53.706+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:32:53.808+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:32:53.806+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:32:53.864+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:32:53.863+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:32:53.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.462 seconds
[2024-07-15T17:33:24.118+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:33:24.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:33:24.129+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:33:24.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:33:24.420+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:33:24.422+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:33:24.667+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:33:24.670+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:33:25.503+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:33:25.570+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:33:25.569+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:33:25.632+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:33:25.631+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:33:25.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.601 seconds
[2024-07-15T17:33:55.926+0000] {processor.py:161} INFO - Started process (PID=185) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:33:55.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:33:55.936+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:33:55.935+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:33:56.178+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:33:56.179+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:33:56.317+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:33:56.319+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:33:57.290+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:33:57.331+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:33:57.330+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:33:57.375+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:33:57.375+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:33:57.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.513 seconds
[2024-07-15T17:34:27.715+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:34:27.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:34:27.723+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:34:27.722+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:34:27.990+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:34:27.992+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:34:28.132+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:34:28.133+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:34:28.696+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:34:28.734+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:34:28.733+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:34:28.774+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:34:28.773+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:34:28.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.133 seconds
[2024-07-15T17:34:59.206+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:34:59.210+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:34:59.214+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:34:59.213+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:34:59.385+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:34:59.387+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:34:59.507+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:34:59.508+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:35:00.080+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:35:00.125+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:35:00.124+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:35:00.204+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:35:00.203+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:35:00.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.062 seconds
[2024-07-15T17:35:30.651+0000] {processor.py:161} INFO - Started process (PID=191) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:35:30.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:35:30.661+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:35:30.660+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:35:30.950+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:35:30.953+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:35:31.169+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:35:31.171+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:35:32.133+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:35:32.206+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:35:32.205+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:35:32.273+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:35:32.272+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:35:32.332+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.712 seconds
[2024-07-15T17:36:02.902+0000] {processor.py:161} INFO - Started process (PID=193) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:36:02.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:36:02.911+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:36:02.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:36:03.169+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:36:03.171+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:36:03.314+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:36:03.315+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:36:04.215+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:36:04.295+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:36:04.294+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:36:04.367+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:36:04.367+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:36:04.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.560 seconds
[2024-07-15T17:36:34.722+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:36:34.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:36:34.731+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:36:34.729+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:36:34.979+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:36:34.980+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:36:35.107+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:36:35.108+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:36:35.791+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:36:35.854+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:36:35.853+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:36:35.913+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:36:35.912+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:36:35.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.290 seconds
[2024-07-15T17:37:06.424+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:37:06.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:37:06.431+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:37:06.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:37:06.651+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:37:06.653+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:37:06.835+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:37:06.837+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:37:07.773+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:37:07.906+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:37:07.905+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:37:08.036+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:37:08.035+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:37:08.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.682 seconds
[2024-07-15T17:37:38.704+0000] {processor.py:161} INFO - Started process (PID=199) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:37:38.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:37:38.715+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:37:38.714+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:37:39.031+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:37:39.034+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:37:39.145+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:37:39.146+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:37:40.036+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:37:40.145+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:37:40.144+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:37:40.238+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:37:40.238+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:37:40.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.636 seconds
[2024-07-15T17:38:10.922+0000] {processor.py:161} INFO - Started process (PID=201) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:38:10.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:38:10.933+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:38:10.931+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:38:11.205+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:38:11.206+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:38:11.312+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:38:11.313+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:38:12.071+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:38:12.114+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:38:12.113+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:38:12.150+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:38:12.149+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:38:12.193+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.298 seconds
[2024-07-15T17:38:42.512+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:38:42.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:38:42.522+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:38:42.521+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:38:42.769+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:38:42.771+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:38:42.919+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:38:42.920+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:38:43.577+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:38:43.689+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:38:43.686+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:38:43.792+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:38:43.790+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:38:43.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.395 seconds
[2024-07-15T17:39:14.191+0000] {processor.py:161} INFO - Started process (PID=205) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:39:14.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:39:14.197+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:39:14.196+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:39:14.371+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:39:14.372+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:39:14.484+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:39:14.485+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:39:15.022+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:39:15.063+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:39:15.063+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:39:15.105+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:39:15.105+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:39:15.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.992 seconds
[2024-07-15T17:39:45.749+0000] {processor.py:161} INFO - Started process (PID=207) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:39:45.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:39:45.757+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:39:45.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:39:46.026+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:39:46.029+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:39:46.225+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:39:46.226+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:39:46.819+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:39:46.891+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:39:46.890+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:39:46.962+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:39:46.961+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:39:47.025+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.307 seconds
[2024-07-15T17:40:17.290+0000] {processor.py:161} INFO - Started process (PID=209) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:40:17.295+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:40:17.301+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:40:17.299+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:40:17.537+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:40:17.538+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:40:17.661+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:40:17.663+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:40:18.259+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:40:18.305+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:40:18.304+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:40:18.352+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:40:18.351+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:40:18.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.146 seconds
[2024-07-15T17:40:48.740+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:40:48.744+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:40:48.750+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:40:48.748+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:40:48.982+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:40:48.983+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:40:49.109+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:40:49.110+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:40:49.771+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:40:49.836+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:40:49.835+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:40:49.909+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:40:49.908+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:40:49.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.256 seconds
[2024-07-15T17:41:20.506+0000] {processor.py:161} INFO - Started process (PID=213) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:41:20.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:41:20.515+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:41:20.514+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:41:20.798+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:41:20.800+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:41:21.020+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:41:21.022+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:41:21.710+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:41:21.750+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:41:21.749+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:41:21.798+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:41:21.798+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:41:21.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.370 seconds
[2024-07-15T17:41:52.007+0000] {processor.py:161} INFO - Started process (PID=215) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:41:52.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:41:52.019+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:41:52.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:41:52.184+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:41:52.186+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:41:52.397+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:41:52.399+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:41:53.633+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:41:53.732+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:41:53.731+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:41:53.837+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:41:53.837+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:41:53.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.954 seconds
[2024-07-15T17:42:24.409+0000] {processor.py:161} INFO - Started process (PID=217) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:42:24.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:42:24.414+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:42:24.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:42:24.529+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:42:24.531+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:42:24.634+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:42:24.635+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:42:25.398+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:42:25.504+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:42:25.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:42:25.592+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:42:25.592+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:42:25.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.234 seconds
[2024-07-15T17:42:56.052+0000] {processor.py:161} INFO - Started process (PID=219) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:42:56.057+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:42:56.068+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:42:56.066+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:42:56.384+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:42:56.386+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:42:56.491+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:42:56.492+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:42:57.322+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:42:57.448+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:42:57.445+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:42:57.565+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:42:57.564+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:42:57.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.602 seconds
[2024-07-15T17:43:28.219+0000] {processor.py:161} INFO - Started process (PID=221) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:43:28.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:43:28.228+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:43:28.227+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:43:28.494+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:43:28.497+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:43:28.658+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:43:28.660+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:43:29.357+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:43:29.419+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:43:29.418+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:43:29.477+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:43:29.477+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:43:29.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.342 seconds
[2024-07-15T17:44:00.168+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:44:00.172+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:44:00.176+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:44:00.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:44:00.437+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:44:00.440+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:44:00.632+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:44:00.633+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:44:01.296+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:44:01.362+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:44:01.362+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:44:01.424+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:44:01.423+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:44:01.476+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.337 seconds
[2024-07-15T17:44:31.965+0000] {processor.py:161} INFO - Started process (PID=225) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:44:31.970+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:44:31.978+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:44:31.975+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:44:32.169+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:44:32.171+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:44:32.312+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:44:32.314+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:44:33.067+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:44:33.128+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:44:33.127+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:44:33.172+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:44:33.172+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:44:33.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.294 seconds
[2024-07-15T17:45:03.286+0000] {processor.py:161} INFO - Started process (PID=227) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:45:03.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:45:03.289+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:45:03.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:45:03.415+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:45:03.416+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:45:03.515+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:45:03.516+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:45:04.058+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:45:04.125+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:45:04.124+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:45:04.190+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:45:04.189+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:45:04.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.995 seconds
[2024-07-15T17:45:34.773+0000] {processor.py:161} INFO - Started process (PID=229) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:45:34.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:45:34.778+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:45:34.777+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:45:34.955+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:45:34.957+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:45:35.166+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:45:35.169+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:45:35.834+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:45:35.894+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:45:35.893+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:45:35.951+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:45:35.950+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:45:36.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.248 seconds
[2024-07-15T17:46:06.412+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:46:06.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:46:06.422+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:46:06.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:46:06.665+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:46:06.667+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:46:06.893+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:46:06.894+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:46:07.443+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:46:07.524+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:46:07.522+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:46:07.592+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:46:07.591+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:46:07.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.266 seconds
[2024-07-15T17:46:37.954+0000] {processor.py:161} INFO - Started process (PID=233) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:46:37.959+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:46:37.965+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:46:37.963+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:46:38.232+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:46:38.234+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:46:38.344+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:46:38.346+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:46:39.040+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:46:39.112+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:46:39.111+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:46:39.160+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:46:39.159+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:46:39.204+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.280 seconds
[2024-07-15T17:47:09.596+0000] {processor.py:161} INFO - Started process (PID=235) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:47:09.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:47:09.607+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:47:09.606+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:47:09.903+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:47:09.906+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:47:10.168+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:47:10.169+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:47:10.887+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:47:10.987+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:47:10.986+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:47:11.081+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:47:11.080+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:47:11.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.594 seconds
[2024-07-15T17:47:41.720+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:47:41.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:47:41.736+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:47:41.734+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:47:42.013+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:47:42.014+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:47:42.133+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:47:42.134+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:47:42.790+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:47:42.871+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:47:42.870+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:47:42.942+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:47:42.941+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:47:42.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.331 seconds
[2024-07-15T17:48:13.583+0000] {processor.py:161} INFO - Started process (PID=239) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:48:13.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:48:13.595+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:48:13.593+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:48:13.908+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:48:13.911+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:48:14.079+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:48:14.081+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:48:14.636+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:48:14.683+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:48:14.683+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:48:14.731+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:48:14.730+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:48:14.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.227 seconds
[2024-07-15T17:48:44.992+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:48:44.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:48:45.000+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:48:44.999+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:48:45.223+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:48:45.225+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:48:45.357+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:48:45.359+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:48:46.002+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:48:46.064+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:48:46.062+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:48:46.143+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:48:46.142+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:48:46.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.247 seconds
[2024-07-15T17:49:16.676+0000] {processor.py:161} INFO - Started process (PID=243) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:49:16.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:49:16.680+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:49:16.679+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:49:16.813+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:49:16.814+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:49:16.915+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:49:16.916+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:49:17.510+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:49:17.555+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:49:17.554+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:49:17.603+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:49:17.603+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:49:17.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.979 seconds
[2024-07-15T17:49:48.378+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:49:48.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:49:48.383+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:49:48.382+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:49:48.640+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:49:48.644+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:49:48.980+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:49:48.981+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:49:50.124+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:49:50.211+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:49:50.210+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:49:50.263+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:49:50.263+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:49:50.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.927 seconds
[2024-07-15T17:50:20.908+0000] {processor.py:161} INFO - Started process (PID=247) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:50:20.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:50:20.918+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:50:20.917+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:50:21.237+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:50:21.240+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:50:21.411+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:50:21.413+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:50:22.246+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:50:22.334+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:50:22.332+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:50:22.447+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:50:22.446+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:50:22.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.657 seconds
[2024-07-15T17:50:53.087+0000] {processor.py:161} INFO - Started process (PID=249) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:50:53.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:50:53.091+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:50:53.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:50:53.502+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:50:53.503+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:50:53.679+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:50:53.685+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:50:54.993+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:50:55.095+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:50:55.090+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:50:55.198+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:50:55.197+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:50:55.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.355 seconds
[2024-07-15T17:51:26.288+0000] {processor.py:161} INFO - Started process (PID=251) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:51:26.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:51:26.292+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:51:26.292+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:51:26.490+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:51:26.493+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:51:26.709+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:51:26.711+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:51:27.450+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:51:27.557+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:51:27.555+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:51:27.665+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:51:27.664+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:51:27.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.470 seconds
[2024-07-15T17:51:58.328+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:51:58.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:51:58.337+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:51:58.336+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:51:58.625+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:51:58.628+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:51:58.773+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:51:58.774+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:51:59.965+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:52:00.137+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:52:00.135+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:52:00.202+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:52:00.201+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:52:00.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.950 seconds
[2024-07-15T17:52:30.865+0000] {processor.py:161} INFO - Started process (PID=255) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:52:30.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:52:30.876+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:52:30.875+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:52:31.192+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:52:31.199+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:52:31.340+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:52:31.341+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:52:31.889+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:52:31.953+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:52:31.952+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:52:32.016+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:52:32.015+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:52:32.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.252 seconds
[2024-07-15T17:53:02.418+0000] {processor.py:161} INFO - Started process (PID=257) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:53:02.423+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:53:02.431+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:53:02.428+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:53:02.724+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:53:02.727+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:53:02.978+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:53:02.980+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:53:04.259+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:53:04.383+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:53:04.381+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:53:04.492+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:53:04.491+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:53:04.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.202 seconds
[2024-07-15T17:53:35.195+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:53:35.200+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:53:35.204+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:53:35.203+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:53:35.491+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:53:35.494+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:53:35.728+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:53:35.730+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:53:36.952+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:53:37.074+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:53:37.072+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:53:37.203+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:53:37.203+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:53:37.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.080 seconds
[2024-07-15T17:54:07.780+0000] {processor.py:161} INFO - Started process (PID=261) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:54:07.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:54:07.790+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:54:07.789+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:54:08.084+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:54:08.087+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:54:08.334+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:54:08.337+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:54:09.529+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:54:09.647+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:54:09.644+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:54:09.771+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:54:09.770+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:54:09.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.109 seconds
[2024-07-15T17:54:40.470+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:54:40.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:54:40.481+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:54:40.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:54:40.782+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:54:40.787+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:54:41.029+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:54:41.032+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:54:42.164+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:54:42.287+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:54:42.286+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:54:42.404+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:54:42.401+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:54:42.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.062 seconds
[2024-07-15T17:55:13.015+0000] {processor.py:161} INFO - Started process (PID=265) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:55:13.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:55:13.023+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:55:13.022+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:55:13.287+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:55:13.290+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:55:13.466+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:55:13.468+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:55:14.147+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:55:14.204+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:55:14.203+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:55:14.292+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:55:14.291+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:55:14.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.392 seconds
[2024-07-15T17:55:45.116+0000] {processor.py:161} INFO - Started process (PID=267) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:55:45.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:55:45.124+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:55:45.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:55:45.230+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:55:45.232+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:55:45.350+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:55:45.351+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:55:45.950+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:55:45.990+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:55:45.990+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:55:46.030+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:55:46.030+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:55:46.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.998 seconds
[2024-07-15T17:56:16.797+0000] {processor.py:161} INFO - Started process (PID=269) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:56:16.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:56:16.806+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:56:16.804+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:56:17.103+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:56:17.106+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:56:17.400+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:56:17.402+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:56:18.381+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:56:18.422+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:56:18.421+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:56:18.474+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:56:18.473+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:56:18.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.788 seconds
[2024-07-15T17:56:49.231+0000] {processor.py:161} INFO - Started process (PID=271) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:56:49.250+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:56:49.262+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:56:49.258+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:56:49.547+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:56:49.548+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:56:49.780+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:56:49.782+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:56:51.084+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:56:51.202+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:56:51.200+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:56:51.352+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:56:51.350+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:56:51.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.268 seconds
[2024-07-15T17:57:21.876+0000] {processor.py:161} INFO - Started process (PID=273) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:57:21.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:57:21.887+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:57:21.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:57:22.265+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:57:22.268+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:57:22.448+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:57:22.449+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:57:23.202+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:57:23.277+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:57:23.276+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:57:23.346+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:57:23.345+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:57:23.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.571 seconds
[2024-07-15T17:57:54.025+0000] {processor.py:161} INFO - Started process (PID=275) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:57:54.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:57:54.034+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:57:54.032+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:57:54.337+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:57:54.341+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:57:54.600+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:57:54.604+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:57:55.596+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:57:55.677+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:57:55.676+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:57:55.753+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:57:55.753+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:57:55.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.836 seconds
[2024-07-15T17:58:26.006+0000] {processor.py:161} INFO - Started process (PID=277) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:58:26.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:58:26.015+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:58:26.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:58:26.325+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:58:26.326+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:58:26.434+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:58:26.436+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:58:27.310+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:58:27.379+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:58:27.378+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:58:27.442+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:58:27.441+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:58:27.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.502 seconds
[2024-07-15T17:58:57.840+0000] {processor.py:161} INFO - Started process (PID=279) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:58:57.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:58:57.846+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:58:57.845+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:58:58.001+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:58:58.003+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:58:58.196+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:58:58.197+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:58:58.861+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:58:58.926+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:58:58.925+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:58:59.009+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:58:59.009+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:58:59.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.354 seconds
[2024-07-15T17:59:29.495+0000] {processor.py:161} INFO - Started process (PID=281) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:59:29.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T17:59:29.506+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:59:29.505+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:59:29.919+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:59:29.920+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:59:30.005+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T17:59:30.006+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T17:59:30.627+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T17:59:30.694+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:59:30.693+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T17:59:30.757+0000] {logging_mixin.py:188} INFO - [2024-07-15T17:59:30.757+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T17:59:30.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.352 seconds
[2024-07-15T18:00:01.315+0000] {processor.py:161} INFO - Started process (PID=283) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:00:01.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:00:01.324+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:00:01.323+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:00:01.670+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:00:01.673+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:00:01.927+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:00:01.929+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:00:03.195+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:00:03.284+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:00:03.283+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:00:03.359+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:00:03.358+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:00:03.442+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.157 seconds
[2024-07-15T18:00:33.894+0000] {processor.py:161} INFO - Started process (PID=285) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:00:33.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:00:33.898+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:00:33.898+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:00:34.027+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:00:34.029+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:00:34.287+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:00:34.289+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:00:35.487+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:00:35.596+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:00:35.595+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:00:35.698+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:00:35.696+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:00:35.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.955 seconds
[2024-07-15T18:01:06.579+0000] {processor.py:161} INFO - Started process (PID=287) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:01:06.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:01:06.589+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:01:06.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:01:06.904+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:01:06.908+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:01:07.145+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:01:07.147+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:01:08.394+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:01:08.492+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:01:08.490+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:01:08.589+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:01:08.588+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:01:08.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.116 seconds
[2024-07-15T18:01:39.110+0000] {processor.py:161} INFO - Started process (PID=289) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:01:39.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:01:39.114+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:01:39.114+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:01:39.268+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:01:39.270+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:01:39.420+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:01:39.421+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:01:40.277+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:01:40.358+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:01:40.356+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:01:40.443+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:01:40.442+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:01:40.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.445 seconds
[2024-07-15T18:02:11.138+0000] {processor.py:161} INFO - Started process (PID=291) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:02:11.143+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:02:11.150+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:02:11.148+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:02:11.479+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:02:11.484+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:02:11.765+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:02:11.767+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:02:12.566+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:02:12.613+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:02:12.611+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:02:12.674+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:02:12.673+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:02:12.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.628 seconds
[2024-07-15T18:02:43.145+0000] {processor.py:161} INFO - Started process (PID=293) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:02:43.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:02:43.153+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:02:43.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:02:43.357+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:02:43.358+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:02:43.480+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:02:43.481+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:02:44.257+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:02:44.342+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:02:44.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:02:44.550+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:02:44.549+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:02:44.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.611 seconds
[2024-07-15T18:03:15.363+0000] {processor.py:161} INFO - Started process (PID=295) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:03:15.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:03:15.401+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:03:15.397+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:03:15.934+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:03:15.937+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:03:16.221+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:03:16.224+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:03:17.395+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:03:17.463+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:03:17.462+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:03:17.572+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:03:17.571+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:03:17.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.316 seconds
[2024-07-15T18:03:47.810+0000] {processor.py:161} INFO - Started process (PID=297) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:03:47.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:03:47.818+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:03:47.817+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:03:48.057+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:03:48.058+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:03:48.203+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:03:48.205+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:03:48.889+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:03:48.949+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:03:48.948+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:03:49.009+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:03:49.009+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:03:49.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.292 seconds
[2024-07-15T18:04:19.655+0000] {processor.py:161} INFO - Started process (PID=299) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:04:19.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:04:19.664+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:04:19.663+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:04:19.937+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:04:19.940+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:04:20.182+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:04:20.184+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:04:21.348+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:04:21.464+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:04:21.462+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:04:21.559+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:04:21.558+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:04:21.650+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.022 seconds
[2024-07-15T18:04:52.215+0000] {processor.py:161} INFO - Started process (PID=301) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:04:52.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:04:52.219+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:04:52.219+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:04:52.348+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:04:52.350+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:04:52.469+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:04:52.471+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:04:53.136+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:04:53.205+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:04:53.204+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:04:53.282+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:04:53.281+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:04:53.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.139 seconds
[2024-07-15T18:05:23.753+0000] {processor.py:161} INFO - Started process (PID=303) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:05:23.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:05:23.766+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:05:23.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:05:24.004+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:05:24.005+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:05:24.119+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:05:24.120+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:05:25.037+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:05:25.162+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:05:25.161+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:05:25.229+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:05:25.229+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:05:25.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.570 seconds
[2024-07-15T18:05:55.898+0000] {processor.py:161} INFO - Started process (PID=305) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:05:55.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:05:55.907+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:05:55.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:05:56.185+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:05:56.189+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:05:56.487+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:05:56.493+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:05:57.043+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:05:57.140+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:05:57.139+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:05:57.226+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:05:57.226+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:05:57.269+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.400 seconds
[2024-07-15T18:06:28.014+0000] {processor.py:161} INFO - Started process (PID=307) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:06:28.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:06:28.024+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:06:28.023+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:06:28.295+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:06:28.299+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:06:28.562+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:06:28.565+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:06:29.448+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:06:29.520+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:06:29.519+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:06:29.591+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:06:29.590+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:06:29.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.662 seconds
[2024-07-15T18:07:00.220+0000] {processor.py:161} INFO - Started process (PID=309) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:07:00.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:07:00.225+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:07:00.224+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:07:00.357+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:07:00.358+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:07:00.471+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:07:00.472+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:07:01.118+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:07:01.172+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:07:01.172+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:07:01.232+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:07:01.231+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:07:01.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.070 seconds
[2024-07-15T18:07:31.771+0000] {processor.py:161} INFO - Started process (PID=311) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:07:31.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:07:31.782+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:07:31.780+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:07:32.099+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:07:32.102+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:07:32.335+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:07:32.336+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:07:33.493+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:07:33.526+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:07:33.525+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:07:33.560+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:07:33.559+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:07:33.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.864 seconds
[2024-07-15T18:08:04.174+0000] {processor.py:161} INFO - Started process (PID=313) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:08:04.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:08:04.183+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:08:04.182+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:08:04.466+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:08:04.470+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:08:04.636+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:08:04.637+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:08:05.213+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:08:05.253+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:08:05.253+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:08:05.290+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:08:05.290+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:08:05.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.179 seconds
[2024-07-15T18:08:35.639+0000] {processor.py:161} INFO - Started process (PID=315) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:08:35.641+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:08:35.642+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:08:35.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:08:35.743+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:08:35.744+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:08:35.934+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:08:35.939+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:08:36.722+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:08:36.778+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:08:36.777+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:08:36.820+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:08:36.820+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:08:36.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.230 seconds
[2024-07-15T18:09:07.172+0000] {processor.py:161} INFO - Started process (PID=317) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:09:07.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:09:07.177+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:09:07.176+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:09:07.318+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:09:07.319+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:09:07.727+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:09:07.734+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:09:08.326+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:09:08.415+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:09:08.414+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:09:08.554+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:09:08.544+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:09:08.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.476 seconds
[2024-07-15T18:09:38.815+0000] {processor.py:161} INFO - Started process (PID=319) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:09:38.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:09:38.821+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:09:38.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:09:39.004+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:09:39.005+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:09:39.152+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:09:39.153+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:09:39.739+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:09:39.855+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:09:39.854+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:09:40.011+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:09:40.010+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:09:40.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.288 seconds
[2024-07-15T18:10:10.507+0000] {processor.py:161} INFO - Started process (PID=321) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:10:10.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:10:10.513+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:10:10.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:10:10.783+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:10:10.785+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:10:11.048+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:10:11.051+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:10:12.198+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:10:12.286+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:10:12.285+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:10:12.340+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:10:12.340+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:10:12.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.909 seconds
[2024-07-15T18:10:43.040+0000] {processor.py:161} INFO - Started process (PID=323) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:10:43.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:10:43.050+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:10:43.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:10:43.481+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:10:43.485+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:10:43.891+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:10:43.893+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:10:45.008+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:10:45.197+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:10:45.195+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:10:45.316+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:10:45.315+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:10:45.406+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.451 seconds
[2024-07-15T18:11:16.056+0000] {processor.py:161} INFO - Started process (PID=325) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:11:16.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:11:16.072+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:11:16.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:11:16.398+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:11:16.400+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:11:16.657+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:11:16.661+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:11:17.788+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:11:17.928+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:11:17.926+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:11:18.056+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:11:18.055+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:11:18.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.145 seconds
[2024-07-15T18:11:48.476+0000] {processor.py:161} INFO - Started process (PID=327) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:11:48.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:11:48.481+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:11:48.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:11:48.572+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:11:48.573+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:11:48.687+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:11:48.688+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:11:49.462+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:11:49.532+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:11:49.531+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:11:49.596+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:11:49.595+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:11:49.650+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.188 seconds
[2024-07-15T18:12:20.246+0000] {processor.py:161} INFO - Started process (PID=329) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:12:20.249+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:12:20.257+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:12:20.255+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:12:20.525+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:12:20.526+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:12:20.651+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:12:20.653+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:12:21.430+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:12:21.551+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:12:21.550+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:12:21.615+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:12:21.614+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:12:21.663+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.457 seconds
[2024-07-15T18:12:52.322+0000] {processor.py:161} INFO - Started process (PID=331) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:12:52.329+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:12:52.333+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:12:52.332+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:12:52.618+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:12:52.622+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:12:52.831+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:12:52.832+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:12:53.569+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:12:53.651+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:12:53.650+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:12:53.741+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:12:53.740+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:12:53.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.528 seconds
[2024-07-15T18:13:24.233+0000] {processor.py:161} INFO - Started process (PID=333) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:13:24.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:13:24.243+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:13:24.241+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:13:24.529+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:13:24.533+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:13:24.791+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:13:24.793+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:13:25.698+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:13:25.813+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:13:25.812+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:13:25.923+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:13:25.922+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:13:26.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.819 seconds
[2024-07-15T18:13:56.635+0000] {processor.py:161} INFO - Started process (PID=335) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:13:56.641+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:13:56.646+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:13:56.645+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:13:56.944+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:13:56.947+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:13:57.241+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:13:57.242+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:13:58.248+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:13:58.327+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:13:58.325+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:13:58.398+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:13:58.397+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:13:58.455+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.861 seconds
[2024-07-15T18:14:28.990+0000] {processor.py:161} INFO - Started process (PID=337) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:14:28.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:14:28.996+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:14:28.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:14:29.293+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:14:29.296+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:14:29.534+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:14:29.537+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:14:30.459+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:14:30.557+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:14:30.556+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:14:30.668+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:14:30.667+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:14:30.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.787 seconds
[2024-07-15T18:15:01.342+0000] {processor.py:161} INFO - Started process (PID=339) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:15:01.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:15:01.348+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:15:01.347+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:15:01.525+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:15:01.526+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:15:01.645+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:15:01.646+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:15:02.176+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:15:02.224+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:15:02.223+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:15:02.268+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:15:02.268+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:15:02.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.980 seconds
[2024-07-15T18:15:32.784+0000] {processor.py:161} INFO - Started process (PID=341) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:15:32.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:15:32.807+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:15:32.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:15:33.134+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:15:33.136+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:15:33.346+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:15:33.348+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:15:34.654+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:15:34.780+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:15:34.779+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:15:34.890+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:15:34.889+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:15:34.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.234 seconds
[2024-07-15T18:16:05.662+0000] {processor.py:161} INFO - Started process (PID=343) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:16:05.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:16:05.671+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:16:05.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:16:05.955+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:16:05.958+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:16:06.097+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:16:06.098+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:16:06.878+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:16:06.960+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:16:06.958+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:16:07.026+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:16:07.026+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:16:07.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.440 seconds
[2024-07-15T18:16:37.629+0000] {processor.py:161} INFO - Started process (PID=345) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:16:37.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:16:37.633+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:16:37.632+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:16:37.833+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:16:37.836+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:16:38.137+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:16:38.140+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:16:39.088+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:16:39.187+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:16:39.186+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:16:39.275+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:16:39.274+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:16:39.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.745 seconds
[2024-07-15T18:17:09.617+0000] {processor.py:161} INFO - Started process (PID=347) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:17:09.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:17:09.622+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:17:09.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:17:09.771+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:17:09.772+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:17:09.971+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:17:09.974+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:17:11.144+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:17:11.190+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:17:11.189+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:17:11.231+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:17:11.231+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:17:11.265+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.685 seconds
[2024-07-15T18:17:41.571+0000] {processor.py:161} INFO - Started process (PID=349) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:17:41.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:17:41.581+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:17:41.580+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:17:41.921+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:17:41.924+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:17:42.088+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:17:42.089+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:17:43.056+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:17:43.151+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:17:43.150+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:17:43.215+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:17:43.215+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:17:43.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.730 seconds
[2024-07-15T18:18:13.825+0000] {processor.py:161} INFO - Started process (PID=351) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:18:13.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:18:13.835+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:18:13.834+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:18:14.115+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:18:14.117+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:18:14.394+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:18:14.398+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:18:15.058+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:18:15.133+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:18:15.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:18:15.198+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:18:15.198+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:18:15.263+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.471 seconds
[2024-07-15T18:18:45.804+0000] {processor.py:161} INFO - Started process (PID=353) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:18:45.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:18:45.817+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:18:45.816+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:18:45.991+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:18:45.992+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:18:46.116+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:18:46.117+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:18:46.825+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:18:46.903+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:18:46.902+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:18:46.949+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:18:46.949+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:18:46.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.206 seconds
[2024-07-15T18:19:17.339+0000] {processor.py:161} INFO - Started process (PID=355) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:19:17.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:19:17.347+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:19:17.346+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:19:17.602+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:19:17.604+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:19:17.841+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:19:17.843+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:19:18.643+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:19:18.709+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:19:18.708+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:19:18.770+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:19:18.769+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:19:18.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.508 seconds
[2024-07-15T18:19:48.981+0000] {processor.py:161} INFO - Started process (PID=357) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:19:48.985+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:19:48.990+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:19:48.989+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:19:49.277+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:19:49.279+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:19:49.488+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:19:49.491+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:19:50.290+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:19:50.350+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:19:50.349+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:19:50.407+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:19:50.406+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:19:50.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.517 seconds
[2024-07-15T18:20:20.852+0000] {processor.py:161} INFO - Started process (PID=359) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:20:20.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:20:20.860+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:20:20.859+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:20:21.071+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:20:21.072+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:20:21.164+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:20:21.165+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:20:21.842+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:20:21.921+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:20:21.920+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:20:21.992+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:20:21.991+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:20:22.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.251 seconds
[2024-07-15T18:20:52.520+0000] {processor.py:161} INFO - Started process (PID=361) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:20:52.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:20:52.536+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:20:52.534+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:20:52.736+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:20:52.737+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:20:52.861+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:20:52.863+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:20:53.377+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:20:53.479+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:20:53.478+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:20:53.572+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:20:53.571+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:20:53.655+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.183 seconds
[2024-07-15T18:21:24.254+0000] {processor.py:161} INFO - Started process (PID=363) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:21:24.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:21:24.262+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:21:24.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:21:24.491+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:21:24.493+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:21:24.639+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:21:24.641+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:21:25.287+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:21:25.327+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:21:25.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:21:25.373+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:21:25.373+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:21:25.420+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.191 seconds
[2024-07-15T18:21:55.688+0000] {processor.py:161} INFO - Started process (PID=365) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:21:55.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:21:55.695+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:21:55.694+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:21:55.858+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:21:55.860+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:21:55.963+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:21:55.965+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:21:56.611+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:21:56.699+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:21:56.696+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:21:56.760+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:21:56.759+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:21:56.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.145 seconds
[2024-07-15T18:22:27.176+0000] {processor.py:161} INFO - Started process (PID=367) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:22:27.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:22:27.185+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:22:27.184+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:22:27.410+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:22:27.412+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:22:27.523+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:22:27.524+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:22:28.086+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:22:28.147+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:22:28.146+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:22:28.202+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:22:28.201+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:22:28.249+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.102 seconds
[2024-07-15T18:22:58.798+0000] {processor.py:161} INFO - Started process (PID=369) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:22:58.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:22:58.807+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:22:58.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:22:59.068+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:22:59.071+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:22:59.230+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:22:59.231+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:22:59.852+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:22:59.930+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:22:59.929+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:22:59.978+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:22:59.977+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:23:00.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.255 seconds
[2024-07-15T18:23:30.300+0000] {processor.py:161} INFO - Started process (PID=371) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:23:30.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:23:30.309+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:23:30.307+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:23:30.546+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:23:30.547+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:23:30.685+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:23:30.687+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:23:31.375+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:23:31.434+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:23:31.434+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:23:31.490+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:23:31.490+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:23:31.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.272 seconds
[2024-07-15T18:24:02.022+0000] {processor.py:161} INFO - Started process (PID=373) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:24:02.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:24:02.030+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:24:02.029+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:24:02.292+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:24:02.295+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:24:02.471+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:24:02.473+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:24:03.066+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:24:03.112+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:24:03.111+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:24:03.153+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:24:03.152+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:24:03.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.210 seconds
[2024-07-15T18:24:33.481+0000] {processor.py:161} INFO - Started process (PID=375) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:24:33.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:24:33.485+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:24:33.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:24:33.635+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:24:33.637+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:24:33.781+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:24:33.783+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:24:34.576+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:24:34.685+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:24:34.684+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:24:34.784+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:24:34.783+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:24:34.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.403 seconds
[2024-07-15T18:25:05.205+0000] {processor.py:161} INFO - Started process (PID=377) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:25:05.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:25:05.215+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:25:05.213+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:25:05.475+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:25:05.478+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:25:05.661+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:25:05.662+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:25:06.585+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:25:06.687+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:25:06.685+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:25:06.773+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:25:06.772+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:25:06.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.654 seconds
[2024-07-15T18:25:37.409+0000] {processor.py:161} INFO - Started process (PID=379) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:25:37.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:25:37.417+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:25:37.416+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:25:37.591+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:25:37.593+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:25:37.721+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:25:37.722+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:25:38.334+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:25:38.414+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:25:38.413+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:25:38.488+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:25:38.488+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:25:38.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.175 seconds
[2024-07-15T18:26:08.839+0000] {processor.py:161} INFO - Started process (PID=381) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:26:08.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:26:08.847+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:26:08.846+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:26:09.115+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:26:09.118+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:26:09.282+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:26:09.284+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:26:09.999+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:26:10.097+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:26:10.095+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:26:10.193+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:26:10.192+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:26:10.269+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.458 seconds
[2024-07-15T18:26:40.688+0000] {processor.py:161} INFO - Started process (PID=383) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:26:40.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:26:40.697+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:26:40.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:26:40.892+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:26:40.893+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:26:41.003+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:26:41.005+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:26:41.593+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:26:41.635+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:26:41.634+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:26:41.688+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:26:41.687+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:26:41.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.083 seconds
[2024-07-15T18:27:12.279+0000] {processor.py:161} INFO - Started process (PID=385) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:27:12.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:27:12.288+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:27:12.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:27:12.526+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:27:12.528+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:27:12.661+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:27:12.663+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:27:13.344+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:27:13.404+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:27:13.403+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:27:13.478+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:27:13.478+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:27:13.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.294 seconds
[2024-07-15T18:27:43.770+0000] {processor.py:161} INFO - Started process (PID=387) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:27:43.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:27:43.784+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:27:43.781+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:27:44.078+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:27:44.081+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:27:44.302+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:27:44.304+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:27:44.918+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:27:45.011+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:27:45.010+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:27:45.109+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:27:45.108+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:27:45.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.442 seconds
[2024-07-15T18:28:15.653+0000] {processor.py:161} INFO - Started process (PID=389) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:28:15.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:28:15.667+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:28:15.664+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:28:16.053+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:28:16.055+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:28:16.261+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:28:16.262+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:28:17.157+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:28:17.240+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:28:17.239+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:28:17.295+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:28:17.294+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:28:17.349+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.797 seconds
[2024-07-15T18:28:47.920+0000] {processor.py:161} INFO - Started process (PID=391) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:28:47.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:28:47.929+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:28:47.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:28:48.168+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:28:48.169+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:28:48.286+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:28:48.288+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:28:49.003+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:28:49.123+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:28:49.122+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:28:49.191+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:28:49.191+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:28:49.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.376 seconds
[2024-07-15T18:34:15.634+0000] {processor.py:161} INFO - Started process (PID=393) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:34:15.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:34:15.701+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:34:15.699+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:34:17.975+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:34:17.978+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:34:49.600+0000] {processor.py:161} INFO - Started process (PID=395) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:34:49.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:34:49.665+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:34:49.663+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:34:51.583+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:34:51.600+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:34:52.649+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:34:52.659+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:34:57.182+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:34:57.592+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:34:57.580+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:34:58.142+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:34:58.130+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:34:58.497+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 9.009 seconds
[2024-07-15T18:35:28.920+0000] {processor.py:161} INFO - Started process (PID=397) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:35:28.922+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:35:28.923+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:35:28.923+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:35:29.016+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:35:29.016+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:35:29.073+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:35:29.074+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:35:29.358+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:35:29.386+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:35:29.385+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:35:29.412+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:35:29.411+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:35:29.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.522 seconds
[2024-07-15T18:35:59.902+0000] {processor.py:161} INFO - Started process (PID=399) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:35:59.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:35:59.911+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:35:59.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:36:00.195+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:36:00.197+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:36:00.413+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:36:00.416+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:36:01.101+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:36:01.184+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:36:01.182+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:36:01.261+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:36:01.260+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:36:01.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.447 seconds
[2024-07-15T18:36:31.834+0000] {processor.py:161} INFO - Started process (PID=401) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:36:31.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:36:31.842+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:36:31.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:36:32.074+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:36:32.075+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:36:32.189+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:36:32.191+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:36:32.678+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:36:32.712+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:36:32.711+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:36:32.750+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:36:32.749+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:36:32.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.989 seconds
[2024-07-15T18:37:03.329+0000] {processor.py:161} INFO - Started process (PID=403) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:37:03.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:37:03.339+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:37:03.337+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:37:03.597+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:37:03.599+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:37:03.756+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:37:03.757+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:37:04.387+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:37:04.446+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:37:04.445+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:37:04.516+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:37:04.516+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:37:04.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.277 seconds
[2024-07-15T18:37:34.718+0000] {processor.py:161} INFO - Started process (PID=405) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:37:34.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:37:34.721+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:37:34.721+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:37:34.836+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:37:34.837+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:37:34.913+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:37:34.914+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:37:35.291+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:37:35.326+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:37:35.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:37:35.362+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:37:35.362+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:37:35.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.685 seconds
[2024-07-15T18:38:05.835+0000] {processor.py:161} INFO - Started process (PID=407) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:38:05.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:38:05.838+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:38:05.837+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:38:05.917+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:38:05.918+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:38:05.978+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:38:05.979+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:38:06.258+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:38:06.310+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:38:06.309+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:38:06.355+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:38:06.355+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:38:06.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.571 seconds
[2024-07-15T18:38:36.834+0000] {processor.py:161} INFO - Started process (PID=409) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:38:36.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:38:36.838+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:38:36.837+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:38:36.940+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:38:36.941+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:38:37.043+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:38:37.044+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:38:37.443+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:38:37.468+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:38:37.468+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:38:37.491+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:38:37.491+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:38:37.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.705 seconds
[2024-07-15T18:39:08.053+0000] {processor.py:161} INFO - Started process (PID=411) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:39:08.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:39:08.058+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:39:08.057+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:39:08.159+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:39:08.160+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:39:08.218+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:39:08.220+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:39:08.526+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:39:08.556+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:39:08.556+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:39:08.582+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:39:08.582+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:39:08.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.579 seconds
[2024-07-15T18:39:39.101+0000] {processor.py:161} INFO - Started process (PID=413) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:39:39.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:39:39.105+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:39:39.105+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:39:39.191+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:39:39.192+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:39:39.287+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:39:39.288+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:39:39.634+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:39:39.677+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:39:39.676+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:39:39.701+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:39:39.701+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:39:39.749+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-15T18:40:09.945+0000] {processor.py:161} INFO - Started process (PID=415) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:40:09.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:40:09.950+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:40:09.949+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:40:10.070+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:40:10.071+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:40:10.132+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:40:10.133+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:40:10.440+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:40:10.480+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:40:10.479+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:40:10.504+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:40:10.504+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:40:10.551+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-15T18:40:41.073+0000] {processor.py:161} INFO - Started process (PID=417) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:40:41.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:40:41.081+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:40:41.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:40:41.316+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:40:41.319+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:40:41.579+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:40:41.580+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:40:42.090+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:40:42.150+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:40:42.149+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:40:42.211+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:40:42.210+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:40:42.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.217 seconds
[2024-07-15T18:41:12.511+0000] {processor.py:161} INFO - Started process (PID=419) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:41:12.514+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:41:12.517+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:41:12.516+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:41:12.676+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:41:12.678+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:41:12.799+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:41:12.800+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:41:13.389+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:41:13.451+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:41:13.450+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:41:13.506+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:41:13.506+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:41:13.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.060 seconds
[2024-07-15T18:41:44.069+0000] {processor.py:161} INFO - Started process (PID=421) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:41:44.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:41:44.078+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:41:44.077+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:41:44.359+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:41:44.362+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:41:44.585+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:41:44.587+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:41:45.656+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:41:45.741+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:41:45.740+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:41:45.812+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:41:45.811+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:41:45.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.827 seconds
[2024-07-15T18:42:16.495+0000] {processor.py:161} INFO - Started process (PID=423) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:42:16.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:42:16.504+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:42:16.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:42:16.765+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:42:16.768+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:42:16.995+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:42:16.997+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:42:18.172+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:42:18.276+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:42:18.275+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:42:18.392+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:42:18.391+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:42:18.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.023 seconds
[2024-07-15T18:42:49.025+0000] {processor.py:161} INFO - Started process (PID=425) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:42:49.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:42:49.039+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:42:49.037+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:42:49.161+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:42:49.162+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:42:49.245+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:42:49.246+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:42:49.798+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:42:49.863+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:42:49.862+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:42:49.916+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:42:49.916+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:42:49.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.949 seconds
[2024-07-15T18:43:20.407+0000] {processor.py:161} INFO - Started process (PID=427) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:43:20.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:43:20.410+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:43:20.410+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:43:20.539+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:43:20.540+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:43:20.624+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:43:20.624+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:43:21.187+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:43:21.238+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:43:21.237+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:43:21.274+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:43:21.274+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:43:21.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.922 seconds
[2024-07-15T18:43:51.957+0000] {processor.py:161} INFO - Started process (PID=429) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:43:51.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:43:51.966+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:43:51.964+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:43:52.200+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:43:52.201+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:43:52.350+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:43:52.351+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:43:53.168+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:43:53.203+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:43:53.203+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:43:53.245+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:43:53.245+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:43:53.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.358 seconds
[2024-07-15T18:44:23.660+0000] {processor.py:161} INFO - Started process (PID=431) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:44:23.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:44:23.671+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:44:23.669+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:44:23.813+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:44:23.814+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:44:23.916+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:44:23.917+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:44:24.450+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:44:24.504+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:44:24.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:44:24.553+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:44:24.552+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:44:24.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.974 seconds
[2024-07-15T18:44:55.094+0000] {processor.py:161} INFO - Started process (PID=433) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:44:55.096+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T18:44:55.099+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:44:55.098+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:44:55.210+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:44:55.211+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:44:55.374+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T18:44:55.376+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T18:44:56.344+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-15T18:44:56.405+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:44:56.404+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-07-15T18:44:56.533+0000] {logging_mixin.py:188} INFO - [2024-07-15T18:44:56.531+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2024-07-15T18:44:56.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.550 seconds
[2024-07-15T19:05:14.301+0000] {processor.py:161} INFO - Started process (PID=435) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-15T19:05:14.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-15T19:05:14.332+0000] {logging_mixin.py:188} INFO - [2024-07-15T19:05:14.320+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-15T19:05:38.550+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T19:05:38.619+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-15T19:05:39.460+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 839, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-15T19:05:39.527+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-16T01:41:06.138+0000] {logging_mixin.py:188} INFO - [2024-07-16T01:41:05.938+0000] {timeout.py:68} ERROR - Process timed out, PID: 435
